{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#        Running Inference on EC2 INF1 in K8 cluster \n",
    "\n",
    "This notebook outlines flow for compiling a pretrained Neural Network Model (Resnet50 using Tensorflow) for inferentia and loading it on K8 cluster with Inferentia nodes for Inference.\n",
    "#### Steps:\n",
    "1. Save the Pretrained model as a pb file\n",
    "2. Compile it for inferentia\n",
    "3. Launch the K8 cluster with Inf1 nodes\n",
    "4. Launch the Tensorflow Model Serving Container\n",
    "5. Run the inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate the model file \n",
    "This steps involves the following:\n",
    "<ul>\n",
    "    <li> Load the model from Keras (FP32) </li>\n",
    "    <li> Optimize the model for inference </li>\n",
    "    <li> Convert the FP32 model to FP16 model </li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Genarate the .pb file from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "model, frozen file, optimized file, input size, input node, output node,\n",
      "resnet50_fp32_keras, resnet50_fp32_keras.pb, resnet50_fp32_keras_opt.pb, 224x224x3, input_1, probs/Softmax\n",
      "probs/Softmax\n",
      "INFO:tensorflow:Restoring parameters from /tmp/resnet50_fp32_keras/resnet50_fp32_keras.ckpt\n",
      "WARNING:tensorflow:From <ipython-input-1-05af2e422984>:50: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 320 variables.\n",
      "INFO:tensorflow:Converted 320 variables to const ops.\n",
      "WARNING:tensorflow:From <ipython-input-1-05af2e422984>:52: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "Done generating resnet50_fp32_keras.pb ......\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from google.protobuf import text_format\n",
    "import tensorflow.python.saved_model\n",
    "\n",
    "# set Keras global configurations\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "float_type = 'float32'\n",
    "float_type2 = 'fp32'\n",
    "tf.keras.backend.set_floatx(float_type)\n",
    "\n",
    "# load pre-trained model using Keras\n",
    "model_name = 'resnet50_%s_keras'%float_type2\n",
    "model = None\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "\n",
    "# various save files\n",
    "frozen_file = model_name + '.pb'\n",
    "opt_file = model_name + '_opt.pb'\n",
    "\n",
    "# obtain parameters\n",
    "model_input = model.input.name.replace(':0', '')\n",
    "model_output = model.output.name.replace(':0', '')\n",
    "batch, height, width, channels = model.input.shape\n",
    "\n",
    "print (\"model, frozen file, optimized file, input size, input node, output node,\")\n",
    "print (\"%s, %s, %s, %dx%dx%d, %s, %s\" %(model_name, frozen_file, opt_file, width, height, channels, model_input, model_output))\n",
    "\n",
    "# obtain the TF session\n",
    "sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "# save checkpoint files for freeze_graph\n",
    "ckpt_file = '/tmp/' + model_name + '/' + model_name + '.ckpt'\n",
    "graph_file = '/tmp/' + model_name + '/' + model_name + '.pb'\n",
    "tf.compat.v1.train.Saver().save(sess, ckpt_file)\n",
    "tf.io.write_graph(sess.graph.as_graph_def(), logdir='.', name=graph_file, as_text=False)\n",
    "\n",
    "print(model_output)\n",
    "with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.compat.v1.train.import_meta_graph(ckpt_file + '.meta')\n",
    "    saver.restore(sess, ckpt_file)\n",
    "    output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "        sess, tf.compat.v1.get_default_graph().as_graph_def(), [model_output])\n",
    "    output_graph_def = tf.compat.v1.graph_util.remove_training_nodes(\n",
    "        output_graph_def, protected_nodes=[model_output])\n",
    "    with open(frozen_file, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "print('Done generating {} ......'.format(frozen_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuron-sdk\t\t    resnet50_fp32_keras.pb\r\n",
      "compiler_workdir\t    resnet50_fp32_keras_opt.pb\r\n",
      "eks-demo-draft.ipynb\t    rn50\r\n",
      "nohup.out\t\t    rn50_fp16\r\n",
      "optimize_for_inference.py   rn50_fp16_compiled_b1_nc4\r\n",
      "pod_manifests\t\t    rn50_fp16_compiled_b1_nc4.zip\r\n",
      "resnet50_fp16_keras_opt.pb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimize the model file for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Generating resnet50_fp32_keras_opt.pb .....\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import node_def_pb2\n",
    "from tensorflow.core.framework import attr_value_pb2\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "\n",
    "def clear_input(node):\n",
    "  for i in range(len(node.input)):\n",
    "    node.input.pop()\n",
    "\n",
    "def replace_name(node, name):\n",
    "  node.name = name\n",
    "     \n",
    "def replace_input(node, input_name, new_name):\n",
    "  # node.input.replace(input_name, new_name)\n",
    "  temp = []\n",
    "  for i in node.input:\n",
    "    temp.extend([new_name if i == input_name else i])\n",
    "  clear_input(node)\n",
    "  for i in temp:\n",
    "    node.input.extend([i])\n",
    "\n",
    "def swap_names(node1, node2):\n",
    "  temp = node2.name\n",
    "  node2.name = node1.name\n",
    "  node1.name = temp\n",
    "\n",
    "def get_const_node(const_node_name, const_by_name):\n",
    "  name = re.sub(\"/read$\", \"\", const_node_name)\n",
    "  return const_by_name[name]\n",
    "\n",
    "def get_const_ndarray(const_node_name, const_by_name):\n",
    "  name = re.sub(\"/read$\", \"\", const_node_name)\n",
    "  node = const_by_name[name]\n",
    "  return tf.make_ndarray(node.attr.get(\"value\").tensor)\n",
    "\n",
    "def adjust_bias_values(bias_node, fbn_node, const_by_name):\n",
    "  bias_val = get_const_ndarray(bias_node.input[1], const_by_name)  \n",
    "  gamma_val = get_const_ndarray(fbn_node.input[1], const_by_name)  \n",
    "  mean_val = get_const_ndarray(fbn_node.input[3], const_by_name)  \n",
    "  variance_val = get_const_ndarray(fbn_node.input[4], const_by_name) \n",
    "  new_bias = bias_val * gamma_val / np.sqrt(variance_val)\n",
    "  new_tensor = tensor_util.make_tensor_proto(new_bias, new_bias.dtype, new_bias.shape)\n",
    "  bias_const_node = get_const_node(bias_node.input[1], const_by_name)\n",
    "  bias_const_node.attr[\"value\"].CopyFrom(attr_value_pb2.AttrValue(tensor=new_tensor))\n",
    "\n",
    "def MoveBiasAddAfterFusedBatchNorm(graphdef):\n",
    "  \"\"\"fold_batch_norm function of TransformGraph is unable to fold Keras ResNet50\n",
    "  because of BiasAdd between Conv2D and FusedBatchNorm (BiasAdd is not needed\n",
    "  if FusedBatchNorm is used, but it exists in Keras ResNet50). Here, we \n",
    "  move BiasAdd to after FusedBatchNorm, and adjust bias value by gamma/sqrt(variance).\n",
    "  \"\"\"\n",
    "  sess = tf.compat.v1.Session(graph=tf.import_graph_def(graphdef))\n",
    "  output_graph_def = tf.compat.v1.GraphDef()\n",
    "  node_by_name = {}\n",
    "  const_by_name = {}\n",
    "  for node in graphdef.node:\n",
    "    # Hack: use FusedBatchNormV2 so fold_batch_norm can recognize\n",
    "    if node.op == \"FusedBatchNormV3\":\n",
    "      node.op = \"FusedBatchNorm\"\n",
    "      del(node.attr[\"U\"])\n",
    "      #import pdb; pdb.set_trace()\n",
    "    copied_node = node_def_pb2.NodeDef()\n",
    "    copied_node.CopyFrom(node)\n",
    "    node_by_name[node.name] = copied_node\n",
    "    skip_add_node = False\n",
    "    # Switch Mul/BiasAdd in Keras RN50 so fold_batch_norm transform would work\n",
    "    if node.op == \"Const\":\n",
    "      const_by_name[node.name] = copied_node  \n",
    "    elif node.op.startswith(\"FusedBatchNorm\"):\n",
    "      inputs = node.input\n",
    "      for i in inputs:\n",
    "        input_node = node_by_name[i]\n",
    "        if input_node.op == \"BiasAdd\":\n",
    "          output_graph_def.node.remove(input_node)\n",
    "          input_node_input0 = input_node.input[0]\n",
    "          # Adjust bias values (multiply by scale/sqrt(variance))\n",
    "          adjust_bias_values(input_node, node, const_by_name)\n",
    "          # Hack: swap names to avoid changing input of activation\n",
    "          swap_names(copied_node, input_node)\n",
    "          # Fix inputs for these two ops\n",
    "          replace_input(copied_node, i, input_node_input0)\n",
    "          replace_input(input_node, input_node_input0, copied_node.name)\n",
    "          # Fix order in node list\n",
    "          output_graph_def.node.extend([copied_node])\n",
    "          output_graph_def.node.extend([input_node])\n",
    "          skip_add_node = True\n",
    "    # Add maybe-modified nodes if not already done\n",
    "    if not skip_add_node:\n",
    "      output_graph_def.node.extend([copied_node])\n",
    "  return output_graph_def\n",
    "\n",
    "def FoldFusedBatchNorm(graph_def):\n",
    "  \"\"\"Optimize training graph for inference:\n",
    "    - Remove Identity and CheckNumerics nodes\n",
    "    - Fold FusedBatchNorm constants into previous Conv2D weights\n",
    "    - Fold other constants\n",
    "    - Strip unused nodes\n",
    "    - Sort by execution order\n",
    "  \"\"\"\n",
    "  transformed_graph_def = TransformGraph (\n",
    "         graph_def,\n",
    "         ['input_1'],\n",
    "         ['probs/Softmax'],\n",
    "         [\n",
    "            'add_default_attributes',\n",
    "            'remove_nodes(op=Identity, op=CheckNumerics)',\n",
    "            'fold_constants(ignore_errors=true)',\n",
    "            'fold_batch_norms',\n",
    "            'fold_old_batch_norms',\n",
    "            'strip_unused_nodes',\n",
    "            'sort_by_execution_order',\n",
    "         ])\n",
    "  return transformed_graph_def\n",
    "\n",
    "def load_graph(model_file):\n",
    "  graph_def = tf.compat.v1.GraphDef()\n",
    "\n",
    "  with open(model_file, \"rb\") as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def\n",
    "\n",
    "\n",
    "in_graph = 'resnet50_fp32_keras.pb'\n",
    "out_graph = 'resnet50_fp32_keras_opt.pb'\n",
    "\n",
    "graph_orig = load_graph(in_graph)\n",
    "graph_mod = MoveBiasAddAfterFusedBatchNorm(graph_orig)\n",
    "graph_mod2 = FoldFusedBatchNorm(graph_mod)\n",
    "with tf.io.gfile.GFile(out_graph, \"wb\") as f:\n",
    "    f.write(graph_mod2.SerializeToString())\n",
    "\n",
    "print('Done Generating {} .....'.format(out_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuron-sdk\t\t    resnet50_fp32_keras.pb\r\n",
      "compiler_workdir\t    resnet50_fp32_keras_opt.pb\r\n",
      "eks-demo-draft.ipynb\t    rn50\r\n",
      "nohup.out\t\t    rn50_fp16\r\n",
      "optimize_for_inference.py   rn50_fp16_compiled_b1_nc4\r\n",
      "pod_manifests\t\t    rn50_fp16_compiled_b1_nc4.zip\r\n",
      "resnet50_fp16_keras_opt.pb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the Model to FP16 .......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea87d87f7acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mout_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resnet50_fp16_keras_opt.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mgraph_f32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mgraph_f16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvertFP32ToOther\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_f32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0moutput_xformed_graph_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_xformed_graph_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ea87d87f7acc>\u001b[0m in \u001b[0;36mConvertFP32ToOther\u001b[0;34m(graphdef)\u001b[0m\n\u001b[1;32m     35\u001b[0m             attr_value_pb2.AttrValue(\n\u001b[1;32m     36\u001b[0m               tensor=tensor_util.make_tensor_proto(a,\\\n\u001b[0;32m---> 37\u001b[0;31m                 dtype=cast_type, shape=a.shape)))\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "from tensorflow.core.framework import node_def_pb2\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "from tensorflow.core.framework import attr_value_pb2\n",
    "from tensorflow.python.framework import tensor_util\n",
    "\n",
    "def ConvertFP32ToOther(graphdef):\n",
    "  \"\"\"Converts an FP32 network by casting all constants (weights) to a lower\n",
    "     precision floating point type (FP16) and updating the dtypes\n",
    "     everywhere.\"\"\"\n",
    "  cast_type = \"float16\"\n",
    "  sess = tf.Session(graph=tf.import_graph_def(graphdef))\n",
    "  output_graph_def = graph_pb2.GraphDef()\n",
    "  dummy_tensor = sess.run(tf.constant([0.1]))\n",
    "  dummy_tensor_proto = tensor_util.make_tensor_proto(dummy_tensor, \\\n",
    "      dtype=cast_type, shape=dummy_tensor.shape)\n",
    "  dummy_tensor32 = sess.run(tf.constant([0.1]))\n",
    "  dummy_tensor_proto32 = tensor_util.make_tensor_proto(dummy_tensor, \\\n",
    "      dtype=tf.float32, shape=dummy_tensor.shape)\n",
    "  dt_float_type_attr = attr_value_pb2.AttrValue(type=dummy_tensor_proto32.dtype)\n",
    "  dt_half_type_attr = attr_value_pb2.AttrValue(type=dummy_tensor_proto.dtype)\n",
    "  for node in graphdef.node:\n",
    "    output_node = node_def_pb2.NodeDef()\n",
    "    output_node.CopyFrom(node)\n",
    "    if (node.op == \"Const\"):\n",
    "      if (node.attr[\"dtype\"] == dt_float_type_attr):\n",
    "        a = tensor_util.MakeNdarray(node.attr[\"value\"].tensor)\n",
    "        a = tf.cast(a, cast_type)\n",
    "        a = sess.run(a)\n",
    "        output_node.attr[\"dtype\"].CopyFrom(dt_half_type_attr)\n",
    "        output_node.attr[\"value\"].CopyFrom(\n",
    "            attr_value_pb2.AttrValue(\n",
    "              tensor=tensor_util.make_tensor_proto(a,\\\n",
    "                dtype=cast_type, shape=a.shape)))\n",
    "    else:\n",
    "      if (\"T\" in node.attr.keys()):\n",
    "        if (output_node.attr[\"T\"] == dt_float_type_attr):\n",
    "          output_node.attr[\"T\"].CopyFrom(dt_half_type_attr)\n",
    "      if (\"Tparams\" in node.attr.keys()):\n",
    "        if (output_node.attr[\"Tparams\"] == dt_float_type_attr):\n",
    "          output_node.attr[\"Tparams\"].CopyFrom(dt_half_type_attr)\n",
    "      if (\"dtype\" in node.attr.keys()):\n",
    "        if (node.attr[\"dtype\"] == dt_float_type_attr):\n",
    "          output_node.attr[\"dtype\"].CopyFrom(dt_half_type_attr)\n",
    "      if (\"SrcT\" in node.attr.keys()):\n",
    "        if (node.attr[\"SrcT\"] == dt_float_type_attr):\n",
    "          output_node.attr[\"SrcT\"].CopyFrom(dt_half_type_attr)\n",
    "      if (\"DstT\" in node.attr.keys()):\n",
    "        if (node.attr[\"DstT\"] == dt_float_type_attr):\n",
    "          output_node.attr[\"DstT\"].CopyFrom(dt_half_type_attr)\n",
    "    output_graph_def.node.extend([output_node])\n",
    "  return output_graph_def\n",
    "\n",
    "def load_graph(model_file):\n",
    "  graph_def = tf.GraphDef()\n",
    "\n",
    "  with open(model_file, \"rb\") as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "  return graph_def\n",
    "\n",
    "print('Converting the Model to FP16 .......')\n",
    "\n",
    "in_graph = 'resnet50_fp32_keras_opt.pb'\n",
    "out_graph = 'resnet50_fp16_keras_opt.pb'\n",
    "graph_f32 = load_graph(in_graph)\n",
    "graph_f16 = ConvertFP32ToOther(graph_f32)\n",
    "output_xformed_graph_name = out_graph\n",
    "with gfile.GFile(output_xformed_graph_name, \"wb\") as f:\n",
    "    f.write(graph_f16.SerializeToString())\n",
    "    \n",
    "print('Done generating {} ......'.format(output_xformed_graph_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compile the model for Inferentia\n",
    "Inferentia supports Ahead of time Compiltaion. The model will be compiled using neuron-cc. The script below captures the essential arguments passed to the neuron-cc compiler. Once the compilation is successful the compiled model will be uploaded to a S3 bucket for later use by the TF Model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import resnet50\n",
    "import tensorflow.neuron as tfn\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "batch_size = 1\n",
    "num_neuroncores = 4\n",
    "\n",
    "def pb_to_saved_model(pb_path, input_names, output_names, model_dir):\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(open(pb_path, 'rb').read())\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        inputs = {name: sess.graph.get_tensor_by_name(ts_name) for name, ts_name in input_names.items()}\n",
    "        outputs = {name: sess.graph.get_tensor_by_name(ts_name) for name, ts_name in output_names.items()}\n",
    "        tf.saved_model.simple_save(sess, model_dir, inputs, outputs)\n",
    "\n",
    "saved_model_dir = \"rn50_fp16\"\n",
    "\n",
    "shutil.rmtree(saved_model_dir, ignore_errors=True)\n",
    "\n",
    "pb_to_saved_model(\"resnet50_fp16_keras_opt.pb\", {\"input_1:0\": \"input_1:0\"}, {\"probs/Softmax:0\" : \"probs/Softmax:0\"}, saved_model_dir)\n",
    "\n",
    "img_arr = np.zeros([batch_size, 224, 224, 3], dtype='float16')\n",
    "compiled_saved_model_dir = saved_model_dir + \"_compiled_b\" + str(batch_size) + \"_nc\" + str(num_neuroncores)\n",
    "shutil.rmtree(compiled_saved_model_dir + \"/1\", ignore_errors=True)\n",
    "\n",
    "print(\"\\n*** Batch size {}, num NeuronCores {} (input shape: {}, saved model dir: {}) ***\\n\".format(batch_size, num_neuroncores, img_arr.shape, compiled_saved_model_dir))\n",
    "compiler_args = ['--batching_en', '--rematerialization_en', '--spill_dis',\n",
    "                 '--sb_size', str((batch_size + 6)*10), \n",
    "                 '--enable-replication', 'True',\n",
    "                 '--num-neuroncores', str(num_neuroncores)]\n",
    "static_weights = False\n",
    "if num_neuroncores >= 8:\n",
    "    compiler_args.append('--static-weights')\n",
    "    static_weights = True\n",
    "\n",
    "shutil.rmtree('compiler_workdir', ignore_errors=True)\n",
    "start = time.time()\n",
    "rslts = tfn.saved_model.compile(saved_model_dir, compiled_saved_model_dir + \"/1\",\n",
    "               model_feed_dict={'input_1:0' : img_arr},\n",
    "               compiler_workdir='compiler_workdir',\n",
    "               dynamic_batch_size=True,\n",
    "               compiler_args = compiler_args)\n",
    "delta = time.time() - start\n",
    "perc_on_inf = rslts['OnNeuronRatio'] * 100\n",
    "\n",
    "compile_success = False\n",
    "if perc_on_inf < 50:\n",
    "    print(\"\\nERROR: Compilation finished in {:.0f} seconds with less than 50% operations placed on Inferentia ({:.1f}%)\\n\".format(delta, perc_on_inf))\n",
    "    if '--static-weights' in compiler_args:\n",
    "        print(\"INFO: Retry compilation without static weights\")\n",
    "        compiler_args.remove('--static-weights')\n",
    "        static_weights = False\n",
    "        shutil.rmtree(compiled_saved_model_dir + \"/1\", ignore_errors=True)\n",
    "        shutil.rmtree('compiler_workdir2', ignore_errors=True)\n",
    "        start = time.time()\n",
    "        rslts = tfn.saved_model.compile(saved_model_dir, compiled_saved_model_dir + \"/1\",\n",
    "                   model_feed_dict={'input_1:0' : img_arr},\n",
    "                   compiler_workdir='compiler_workdir2',\n",
    "                   dynamic_batch_size=True,\n",
    "                   compiler_args = compiler_args)\n",
    "        delta = time.time() - start\n",
    "        perc_on_inf = rslts['OnNeuronRatio'] * 100\n",
    "        if perc_on_inf < 50:\n",
    "            print(\"\\nERROR: Retry compilation finished in {:.0f} seconds with less than 50% operations placed on Inferentia ({:.1f}%)\\n\".format(delta, perc_on_inf))\n",
    "        else:    \n",
    "            print(\"\\nINFO: Retry compilation finished in {:.0f} seconds with {:.1f}% operations placed on Inferentia\\n\".format(delta, perc_on_inf))\n",
    "            compile_success = True\n",
    "else:    \n",
    "    print(\"\\nINFO: Compilation finished in {:.0f} seconds with {:.1f}% operations placed on Inferentia\\n\".format(delta, perc_on_inf))\n",
    "    compile_success = True\n",
    "\n",
    "# Prepare SavedModel for uploading to Inf1 instance\n",
    "completion_code = 0\n",
    "if compile_success:\n",
    "    shutil.make_archive('./' + compiled_saved_model_dir, 'zip', './', compiled_saved_model_dir)\n",
    "    completion_code = 1 + int(static_weights)\n",
    "\n",
    "print(completion_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p rn50/1\n",
    "!cp rn50_fp16/saved_model.pb rn50/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload the compied model to S3 for TF serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync rn50 s3://eks-tests/rn50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://eks-tests/rn50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launch the K8 cluster with INF1 nodes\n",
    "\n",
    "#### Steps:\n",
    "  1. Launch the cluster\n",
    "  2. Apply neuron device plugin as a daemon set. [This device plugin is reponsible for exposing the neuron devices to containers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch the K8s INF1 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!eksctl create cluster --name=eks-demo-inf1 \\\n",
    "                       --nodes=1 \\\n",
    "                       --node-ami=ami-0b7eb206d4a5ad3ea \\ #remove this once the EKS INF1 is announced\n",
    "                       --node-type=inf1.2xlarge \\\n",
    "                       --ssh-access \\\n",
    "                       --region=us-west-2 \\\n",
    "                       --ssh-public-key ~/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/demo_k8s_inf1/pod_manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat k8s-neuron-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply neuron device plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f k8s-neuron-device-plugin.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the secret.yml with AWS credentals required to access S3 bucket from worker nodes\n",
    "!kubectl apply -f secret.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launch the Tensorflow Model Server\n",
    "\n",
    "The model server POD should host two containers\n",
    "1. TF-SERVING container, that performs the follwoing\n",
    "    a. Pulls the compiled model from s3 and lods the model into inferentia neuron core by calling neuron-rtd GRPC APIs\n",
    "    b. Exposes a north bound Interface for the clinets to run the inference\n",
    "2. neuron-rtd side car container that manages the inferentia device\n",
    "\n",
    "It is recommended to keep both neuron-rtd and Model serving contaner in the same POD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat rn50_service.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f rn50_service.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl port-forward svc/inf-k8s-test 9000:9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get svc -A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "print('Starting the Inference.........')\n",
    "channel = grpc.insecure_channel('localhost:9000')\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "img_file = tf.keras.utils.get_file(\n",
    "        \"./kitten_small.jpg\",\n",
    "        \"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\")\n",
    "\n",
    "img = image.load_img(img_file, target_size=(224, 224))\n",
    "img_array = preprocess_input(image.img_to_array(img)[None, ...])\n",
    "    \n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'rn50_fp16'\n",
    "\n",
    "#cast the input to fp16\n",
    "img_array = img_array.astype(np.float16)\n",
    "request.inputs['input_1:0'].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(img_array, shape=img_array.shape))\n",
    "result = stub.Predict(request)\n",
    "prediction = tf.make_ndarray(result.outputs['probs/Softmax:0'])\n",
    "res = decode_predictions(prediction)[0][0][1]\n",
    "if res == 'tabby':\n",
    "    print(\"Infer Test Passed..\")\n",
    "else:\n",
    "    print('Inference result missmatch. Expected tabby got %s' % res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_tensorflow_p36)",
   "language": "python",
   "name": "conda_aws_neuron_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
